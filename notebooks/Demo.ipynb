{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56ba2387-54e8-4aa5-aedb-c7d90644536f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# NASA Earthdata API Client 🌍\n",
    "\n",
    "## Overview\n",
    "\n",
    "> TL;DR: **earthaccess** is uses NASA APIs to search, preview and access NASA datasets on-prem and in the cloud with 4 lines of Python.\n",
    "\n",
    "There are many ways to access NASA datasets, we can use the Earthdata search portal. We can use DAAC specific portals or tools.\n",
    "We could even use data.gov! These web portals are great but... they are not designed for programmatic access and reproducible workflows. \n",
    "This is extremely important in the age of the cloud and reproducible open science.\n",
    "\n",
    "The good news is that NASA also exposes APIs that allows us to search, transform and access data in a programmatic way. \n",
    "There are already some very useful client libraries for these APIs:\n",
    "\n",
    "* python-cmr\n",
    "* eo-metadata-tools\n",
    "* harmony-py\n",
    "* Hyrax (OpenDAP)\n",
    "* cmr-stac\n",
    "* others\n",
    "\n",
    "Each of these libraries has amazing features and some similarities. \n",
    "* [cmr-stac](https://medium.com/pangeo/intake-stac-nasa-4cd78d6246b7) is probably the best option for a streamlined workflow from dataset search and discovery to efficiently loading data using python libraries like pandas or xarray.\n",
    "* [*Harmony-py*](https://harmony.earthaccess.nasa.gov/) is the more capable client if we want to pre process the data beforehand(reformat NetCDF to Zarr, reproject, subset). Unfortunately not all datasets are yet covered by Haromny.\n",
    "\n",
    "In this context, **earthaccess** aims to be a simple library that can deal with the important parts of the metadata so we can access or download data without having to worry if a given dataset is on-prem or in the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6aed5ed-1f7b-46ac-b076-3278b5bc20cd",
   "metadata": {},
   "source": [
    "## Querying for data collections\n",
    "The DataCollection client can query CMR for any collection using all of CMR's Query parameters and has built-in accessors for the common ones.\n",
    "This makes it ideal for one liners and easier notation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7372e6-4495-4191-a377-dbcee15065f0",
   "metadata": {},
   "source": [
    "### NASA EDL and the Auth class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acb1926-3442-446a-8a35-fcbba8868d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the classes from earthaccess\n",
    "from earthaccess import Auth, DataCollections, DataGranules, Store\n",
    "# auth = Auth().login(strategy=\"interactive\")\n",
    "auth = Auth().login(strategy=\"netrc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dca47d-0281-432b-a606-541da289d3e1",
   "metadata": {},
   "source": [
    "### Auth\n",
    "\n",
    "The core function of Auth is to deal with cloud credentials and in some cases CMR authenticated queries. \n",
    "If we belong to an early adopter group within NASA we can pass the Auth object to the other classes when we instantiate them.\n",
    "\n",
    "```python\n",
    "# An anonymous query to CMR\n",
    "Query = DataCollections().keyword('elevation')\n",
    "# An authenticated query to CMR\n",
    "Query = DataCollections(auth).keyword('elevation')\n",
    "```\n",
    "\n",
    "and it's the same with DataGranules\n",
    "\n",
    "\n",
    "```python\n",
    "# An anonymous query to CMR\n",
    "Query = DataGranules().keyword('elevation')\n",
    "# An authenticated query to CMR\n",
    "Query = DataGranules(auth).keyword('elevation')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5bf4c9-571b-4c93-af94-e66bd51cb584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now search for collections using a pythonic API client for CMR.\n",
    "# Query = DataCollections(auth).keyword('fire').temporal(\"2016-01-01\", \"2020-12-12\")\n",
    "# Query = DataCollections(auth).keyword('GEDI').bounding_box(-134.7,58.9,-133.9,59.2)\n",
    "\n",
    "Query = DataCollections().keyword('elevation').bounding_box(-134.7,58.9,-133.9,59.2)\n",
    "\n",
    "print(f'Collections found: {Query.hits()}')\n",
    "\n",
    "# filtering what UMM fields to print, to see the full record we omit the fields filters\n",
    "# meta is always included as \n",
    "collections = Query.fields(['ShortName','Abstract']).get(10)\n",
    "# Inspect 5 results printing just the ShortName and Abstract\n",
    "collections[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb5154c-f131-44ad-a68f-cf0fa21ce18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the results from DataCollections and DataGranules are enhanced python dict objects, we still can get all the fields from CMR\n",
    "collections[0][\"umm.ShortName\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d45a6f-ac37-4744-bcfe-88ac3dd6ac07",
   "metadata": {},
   "source": [
    "The DataCollections class returns python dictionaries with some handy methods.\n",
    "\n",
    "```python \n",
    "collection.concept_id() # returns the concept-id, used to search for data granules\n",
    "collection.abstract() # returns the abstract\n",
    "collection.landing_page() # returns the landing page if present in the UMM fields\n",
    "collection.get_data() # returns the portal where data can be accessed.\n",
    "```\n",
    "\n",
    "The same results can be obtained using the `dict` syntax:\n",
    "\n",
    "```python\n",
    "collection[\"meta\"][\"concept-id\"] # concept-id\n",
    "collection[\"umm\"][\"RelatedUrls\"] # URLs, with GET DATA, LANDING PAGE etc\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cdcd74-dfe3-4b83-93f4-7378a0d981df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now search for collections using a pythonic API client for CMR.\n",
    "Query = DataCollections().daac(\"PODAAC\")\n",
    "\n",
    "print(f'Collections found: {Query.hits()}')\n",
    "collections = Query.fields(['ShortName']).get(20)\n",
    "# Printing 3 collections\n",
    "collections[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63792353-ab3e-4f0b-963d-7750e4b89113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we want cloud collections\n",
    "Query = DataCollections().daac(\"PODAAC\").cloud_hosted(True)\n",
    "\n",
    "print(f'Collections found: {Query.hits()}')\n",
    "collections = Query.fields(['ShortName']).get(20)\n",
    "# Printing 3 collections\n",
    "collections[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c5a34a-e808-4cc9-b34d-353d091a8242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the concept-id for the first 10 collections\n",
    "[collection.concept_id() for collection in collections[0:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb9c3bb-ac8b-48e8-8233-8c44da8fb7bc",
   "metadata": {},
   "source": [
    "## Querying for data granules\n",
    "\n",
    "The DataGranules class provides similar functionality as the collection class. To query for granules in a more reliable way concept-id would be the main key.\n",
    "You can search data granules using a short name but that could (more likely will) return different versions of the same data granules. \n",
    "\n",
    "In this example we're querying for 20 data grnaules from ICESat-2  [ATL05](https://nsidc.org/data/ATL03/versions/) version `005` dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9364d737-5a79-4089-853f-76d2ad1c85a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generally speaking we won't need the auth instance for queries to collections and granules\n",
    "# Query = DataGranules().short_name('ATL03').version(\"005\").bounding_box(-134.7,58.9,-133.9,59.2)\n",
    "\n",
    "Query = DataGranules().short_name('ATL03').version(\"005\").bounding_box(-134.7,58.9,-133.9,59.2)\n",
    "granules = Query.get(20)\n",
    "print(granules[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aa8035-f4a8-4592-b19c-49d5c06331fb",
   "metadata": {},
   "source": [
    "## Pretty printing data granules\n",
    "\n",
    "Since we are in a notebook we can take advantage of it to see a more user friendly version of the granules with the built-in function `display`\n",
    "This will render browse image for the granule if available and eventually will have a similar representation as the one from the Earthdata search portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cd5f5c-a854-4a72-a831-33b8bd7ce9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing 2 granules using display\n",
    "[display(granule) for granule in granules[0:2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8f32a4-026d-4a5f-af66-69026cabe966",
   "metadata": {},
   "source": [
    "### Spatiotemporal queries\n",
    "\n",
    "Our granules and collection classes accept the same spatial and temporal arguments as CMR so we can search for granules that match spatiotemporal criteria.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aa39ec-e2fb-49d1-bc54-8d8a2f0655aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Query = DataGranules().short_name(\"ATL03\").temporal(\"2020-03-01\", \"2020-03-30\").bounding_box(-134.7,58.9,-133.9,59.2).version(\"005\")\n",
    "# Always inspects the hits before retrieven the granule metadata, just because it's very verbose.\n",
    "print(f\"Granules found: {Query.hits()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c493585-0d48-41bb-8815-6c83ad20ae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can print some info about these granules using the built-in methods\n",
    "granules = Query.get(4)\n",
    "data_links = [{'links': g.data_links(), 'size (MB):': g.size()} for g in granules]\n",
    "data_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f3526f-4255-43e4-b5e2-7f926216e27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More datasets to try\n",
    "\n",
    "# C1908348134-LPDAAC_ECS: GEDI L2A Elevation and Height Metrics Data Global Footprint Level V002\n",
    "# C1968980609-POCLOUD: Sentinel-6A MF Jason-CS L2 P4 Altimeter Low Resolution (LR) STC Ocean Surface Topography\n",
    "# C1575731655-LPDAAC_ECS: ASTER Global Digital Elevation Model NetCDF V003\n",
    "# Query = DataGranules(auth).short_name('ATL03').version(\"005\")\n",
    "Query = DataGranules().short_name('ATL03').version(\"005\").bounding_box(-134.7,58.9,-133.9,59.2)\n",
    "# Query = DataGranules().concept_id(\"C1575731655-LPDAAC_ECS\").bounding_box(-134.7,58.9,-133.9,59.2)\n",
    "print(f\"Granules found: {Query.hits()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eafb81a-07da-439b-8daa-d23f55d6f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not all granules have data previews, if they have the granule class will show up to 2 preview images while using Jupyter's display() function\n",
    "granules = Query.get(10)\n",
    "[display(g) for g in granules[0:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09094e5-3974-40c0-a99f-99f59d383cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Granules are python dictionaries, with fancy nested key/value notation and some extra built-in methods.\n",
    "granules[0][\"umm.TemporalExtent.RangeDateTime\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a344149-8718-4dc6-b962-75409318a601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size in MB\n",
    "data_links = [{'links': g.data_links(), 'size (MB):': g.size()} for g in granules]\n",
    "data_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c90c43-6e17-42f5-8bf5-95fdd3cb0dce",
   "metadata": {},
   "source": [
    "## **Accessing the data**\n",
    "\n",
    "The cloud is not something magical, but having infrastructure on-demand is quite handy to have on many scientific workflows, especially if the data already lives in \"the cloud\".\n",
    "As for NASA, a data migration started in 2020 and will continue on the foreseeable future. Not all but most of NASA data will be available on AWS object storage system or S3.\n",
    "\n",
    "To work with this data the first thing we need to do is to get the proper credentials for accessing data on their S3 buckets. These credentials are on a per-DAAC base and last a mere 1 hour. In the near future the Auth class will keep track of this to regenerate the credentials as needed.\n",
    "\n",
    "With `earthaccess` a researcher can get the files regardless if they are on-prem or cloud based with the same API call, although an important consideration is that if we want to access data in the cloud we must run the code in the cloud. This is because some S3 buckets are configured to only allow direct access (s3:// links) if the requester is in the same zone, `us-west-2`.\n",
    "\n",
    "## On-prem access  📡\n",
    "\n",
    "DAAC hosted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b796fa53-60ac-4197-922d-1f6ee5dec00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to start the notebook from here we need to execute this cell and uncomment the lines below\n",
    "# Accessing not necessarily means downloading, specially in the cloud.\n",
    "from earthaccess import Auth, DataGranules, DataCollections, Store\n",
    "auth = Auth().login(strategy=\"netrc\")\n",
    "store = Store(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910e4b90-f0e0-42e5-a4e2-d5444089161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Query = DataGranules().concept_id(\"C1997321091-NSIDC_ECS\").bounding_box(-134.7,54.9,-100.9,69.2)\n",
    "print(f\"Granule hits: {Query.hits()}\")\n",
    "# getting more than 6,000 metadata records for demo purposes is going to slow us down a bit so let's get only 100\n",
    "granules = Query.get(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68779b52-8c14-438f-9427-50ce0038eb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "granules[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e6090d-481d-4f80-acef-8d55be6bfaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does this granule belong to a cloud-based collection?\n",
    "granules[0].cloud_hosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88ee4c9-5b8e-4633-aa7f-f323fd0ed283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the response is an array of dictionaries we can do pythonic things like ordering the granules by size\n",
    "import operator\n",
    "granules_by_size = sorted(granules, key=operator.itemgetter(\"size\"))\n",
    "# now our array is sorted by size from less to more. Let's print the first 10\n",
    "granules_by_size[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434466a3-602b-4dff-a260-f7db6901514a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# accessing the data on prem means downloading it if we are in a local environment or \"uploading them\" if we are in the cloud.\n",
    "files = store.get(granules_by_size[0:2], \"./data/demo-atl03\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe45fff-68ea-4f01-94c7-416d79cfd84c",
   "metadata": {},
   "source": [
    "## Cloud access ☁️\n",
    "\n",
    "Same API, just a different place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44403d51-0aa3-423c-8fff-e40d4969aa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Query = DataGranules().concept_id(\"C1968980609-POCLOUD\").bounding_box(-134.7,54.9,-100.9,69.2)\n",
    "print(f\"Granule hits: {Query.hits()}\")\n",
    "cloud_granules = Query.get(100)\n",
    "# is this a cloud hosted data granule?\n",
    "cloud_granules[0].cloud_hosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e59ca3e-b5d5-490f-b967-01d1c7b3fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pretty print this\n",
    "print(cloud_granules[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142739a4-409a-4d98-8978-43aa221b09f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_links = cloud_granules[0].data_links(s3_only=True)\n",
    "https_links = []\n",
    "s3_links = []\n",
    "\n",
    "for granule in cloud_granules[0:10]:\n",
    "    https_links.append(granule.data_links()[0])\n",
    "    s3_links.append(granule.data_links(s3_only=True)[0])\n",
    "https_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3afc1a-145e-4084-9c93-dd9e1eff3c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's order them by size again.\n",
    "import operator\n",
    "cloud_granules_by_size = sorted(cloud_granules, key=operator.itemgetter(\"size\"))\n",
    "# now our array is sorted by size from less to more. Let's print the first 10\n",
    "cloud_granules_by_size[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a294f1-b1f9-4cd4-8751-dfc32feacec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# If we get an error with direct_access=True, most likely is because we are running this code outside the us-west-2 region.\n",
    "# Downloading cloud collection outside us-west-2 causes egress costs to NASA.\n",
    "try:\n",
    "    files = store.get(https_links,\n",
    "                      direct_access=False,\n",
    "                      local_path=\"./data/demo-POCLOUD\")\n",
    "except Exception as e:\n",
    "    # We're probably not in us-west-2\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677dbc16-aa3d-4af1-ada6-d2ab32977ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# If we get an error with direct_access=True, most likely is because we are running this code outside the us-west-2 region.\n",
    "# Downloading cloud collection outside us-west-2 causes egress costs to NASA.\n",
    "try:\n",
    "    files = store.get(cloud_granules_by_size[0:3], direct_access=True, local_path=\"./data/demo-POCLOUD\")\n",
    "except Exception as e:\n",
    "    # We're probably not in us-west-2\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d276eca3-ca27-4970-9367-7e65e5f2302f",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "```python\n",
    "from earthaccess import Auth, DataGranules, DataCollections, Accessor\n",
    "auth = Auth().login()\n",
    "access = Accessor(auth)\n",
    "\n",
    "Query = DataGranules(auth).concept_id(\"C1575731655-LPDAAC_ECS\").bounding_box(-134.7,58.9,-133.9,59.2)\n",
    "granules = Query.get(10)\n",
    "# preview the data granules\n",
    "granules \n",
    "# get the files\n",
    "files = access.get(granules)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "**Wait, we said 4 lines of Python**\n",
    "\n",
    "```python\n",
    "\n",
    "from earthaccess import Auth, DataGranules, Accessor\n",
    "auth = Auth().login()\n",
    "granules = DataGranules().concept_id(\"C1575731655-LPDAAC_ECS\").temporal(\"2020-03-01\", \"2020-03-30\").bounding_box(-134.7,58.9,-133.9,59.2).get_all()\n",
    "files = Accessor(auth).get(granules, local_path='./data')\n",
    "\n",
    "# Now to the important science!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d779e877-2f0c-4da2-92d5-6cc299204956",
   "metadata": {},
   "source": [
    "### Related links\n",
    "\n",
    "**CMR** API documentation: https://cmr.earthaccess.nasa.gov/search/site/docs/search/api.html\n",
    "\n",
    "**EDL** API documentation: https://urs.earthaccess.nasa.gov/\n",
    "\n",
    "NASA OpenScapes: https://nasa-openscapes.github.io/earthaccess-cloud-cookbook/\n",
    "\n",
    "NSIDC: https://nsidc.org"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
